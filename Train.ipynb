{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from __future__ import print_function\r\n",
    "import tensorflow as tf\r\n",
    "import keras\r\n",
    "from keras.preprocessing.image import ImageDataGenerator\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\r\n",
    "from keras.layers import Conv2D,MaxPooling2D\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Read the the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "num_classes=2\r\n",
    "img_rows,img_cols=48,48\r\n",
    "batch_size=64\r\n",
    "\r\n",
    "train_data_dir='data_set/train'\r\n",
    "validation_data_dir='data_set/validation'\r\n",
    "\r\n",
    "train_datagen = ImageDataGenerator(\r\n",
    "\t\t\t\t\trescale=1./255,\r\n",
    "\t\t\t\t\trotation_range=30,\r\n",
    "\t\t\t\t\tshear_range=0.3,\r\n",
    "\t\t\t\t\tzoom_range=0.3,\r\n",
    "\t\t\t\t\twidth_shift_range=0.4,\r\n",
    "\t\t\t\t\theight_shift_range=0.4,\r\n",
    "\t\t\t\t\thorizontal_flip=True,\r\n",
    "\t\t\t\t\tfill_mode='nearest')\r\n",
    "\r\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\r\n",
    "\r\n",
    "train_generator = train_datagen.flow_from_directory(\r\n",
    "\t\t\t\t\ttrain_data_dir,\r\n",
    "\t\t\t\t\tcolor_mode='grayscale',\r\n",
    "\t\t\t\t\ttarget_size=(img_rows,img_cols),\r\n",
    "\t\t\t\t\tbatch_size=batch_size,\r\n",
    "\t\t\t\t\tclass_mode='categorical',\r\n",
    "\t\t\t\t\tshuffle=True)\r\n",
    "\t\t\t\t\t\r\n",
    "validation_generator = validation_datagen.flow_from_directory(\r\n",
    "\t\t\t\t\t\t\tvalidation_data_dir,\r\n",
    "\t\t\t\t\t\t\tcolor_mode='grayscale',\r\n",
    "\t\t\t\t\t\t\ttarget_size=(img_rows,img_cols),\r\n",
    "\t\t\t\t\t\t\tbatch_size=batch_size,\r\n",
    "\t\t\t\t\t\t\tclass_mode='categorical',\r\n",
    "\t\t\t\t\t\t\tshuffle=True)\r\n",
    "\r\n",
    "model = Sequential()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 71 images belonging to 2 classes.\n",
      "Found 22 images belonging to 2 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Fit the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#Block-1\r\n",
    "\r\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\r\n",
    "model.add(Activation('elu'))\r\n",
    "model.add(BatchNormalization())\r\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',input_shape=(img_rows,img_cols,1)))\r\n",
    "model.add(Activation('elu'))\r\n",
    "model.add(BatchNormalization())\r\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "\r\n",
    "#Block-2 \r\n",
    "\r\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\r\n",
    "model.add(Activation('elu'))\r\n",
    "model.add(BatchNormalization())\r\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\r\n",
    "model.add(Activation('elu'))\r\n",
    "model.add(BatchNormalization())\r\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "\r\n",
    "#Block-3\r\n",
    "\r\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\r\n",
    "model.add(Activation('elu'))\r\n",
    "model.add(BatchNormalization())\r\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\r\n",
    "model.add(Activation('elu'))\r\n",
    "model.add(BatchNormalization())\r\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "\r\n",
    "#Block-4 \r\n",
    "\r\n",
    "# model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\r\n",
    "# model.add(Activation('elu'))\r\n",
    "# model.add(BatchNormalization())\r\n",
    "# model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\r\n",
    "# model.add(Activation('elu'))\r\n",
    "# model.add(BatchNormalization())\r\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
    "# model.add(Dropout(0.2))\r\n",
    "\r\n",
    "#Block-5\r\n",
    "\r\n",
    "model.add(Flatten())\r\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\r\n",
    "model.add(Activation('elu'))\r\n",
    "model.add(BatchNormalization())\r\n",
    "model.add(Dropout(0.5))\r\n",
    "\r\n",
    "#Block-6\r\n",
    "\r\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\r\n",
    "model.add(Activation('elu'))\r\n",
    "model.add(BatchNormalization())\r\n",
    "model.add(Dropout(0.5))\r\n",
    "\r\n",
    "#Block-7\r\n",
    "\r\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\r\n",
    "model.add(Activation('softmax'))\r\n",
    "\r\n",
    "print(model.summary())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 588,002\n",
      "Trainable params: 586,850\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Train the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\r\n",
    "from tensorflow.keras.optimizers import SGD\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\r\n",
    "from time import time\r\n",
    "\r\n",
    "# checkPointName = 'EmotionDetectionModel.h5'\r\n",
    "#quick fix to increment the file if it exists\r\n",
    "counter = 0\r\n",
    "filename = \"EmotionDetectionModel{}.h5\"\r\n",
    "while os.path.isfile(filename.format(counter)):\r\n",
    "    counter += 1\r\n",
    "checkPointName = filename.format(counter)\r\n",
    "#quick fix to increment the file if it exists\r\n",
    "\r\n",
    "checkpoint = ModelCheckpoint(checkPointName,\r\n",
    "                             monitor='val_loss',\r\n",
    "                             mode='min',\r\n",
    "                             verbose=1)\r\n",
    "\r\n",
    "earlystop = EarlyStopping(monitor='accuracy',\r\n",
    "                          min_delta=0,\r\n",
    "                          patience=5000,\r\n",
    "                          verbose=1,\r\n",
    "                          restore_best_weights=True\r\n",
    "                          )\r\n",
    "\r\n",
    "reduce_lr = ReduceLROnPlateau(monitor='accuracy',\r\n",
    "                              factor=0.2,\r\n",
    "                              patience=5000,\r\n",
    "                              verbose=1,\r\n",
    "                              min_delta=0.0001)\r\n",
    "\r\n",
    "# Create a TensorBoard instance with the path to the logs directory\r\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\r\n",
    "\r\n",
    "callbacks = [earlystop,checkpoint,reduce_lr, tensorboard]\r\n",
    "''', tensorboard'''\r\n",
    "\r\n",
    "model.compile(loss='categorical_crossentropy',\r\n",
    "              optimizer = Adam(lr=0.001),\r\n",
    "              metrics=['accuracy'])\r\n",
    "\r\n",
    "nb_train_samples = 71\r\n",
    "nb_validation_samples = 22\r\n",
    "epochs=100\r\n",
    "\r\n",
    "history=model.fit_generator(\r\n",
    "                train_generator,\r\n",
    "                steps_per_epoch=nb_train_samples//batch_size,\r\n",
    "                epochs=epochs,\r\n",
    "                callbacks=callbacks,\r\n",
    "                validation_data=validation_generator,\r\n",
    "                validation_steps=nb_validation_samples//batch_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 3s 1s/step - loss: 1.4966 - accuracy: 0.4359\n",
      "\n",
      "Epoch 00001: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 1.0058 - accuracy: 0.6094\n",
      "\n",
      "Epoch 00002: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.9082 - accuracy: 0.6410\n",
      "\n",
      "Epoch 00003: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 1s 139ms/step - loss: 0.5714 - accuracy: 0.7179\n",
      "\n",
      "Epoch 00004: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 377ms/step - loss: 1.3189 - accuracy: 0.5641\n",
      "\n",
      "Epoch 00005: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 1s 130ms/step - loss: 0.9022 - accuracy: 0.7436\n",
      "\n",
      "Epoch 00006: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 355ms/step - loss: 1.0476 - accuracy: 0.6154\n",
      "\n",
      "Epoch 00007: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 1s 359ms/step - loss: 0.7953 - accuracy: 0.6562\n",
      "\n",
      "Epoch 00008: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 362ms/step - loss: 0.5581 - accuracy: 0.7436\n",
      "\n",
      "Epoch 00009: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.7419 - accuracy: 0.7436\n",
      "\n",
      "Epoch 00010: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 1s 329ms/step - loss: 0.6470 - accuracy: 0.7656\n",
      "\n",
      "Epoch 00011: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 354ms/step - loss: 0.6666 - accuracy: 0.6923\n",
      "\n",
      "Epoch 00012: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 1s 133ms/step - loss: 0.8028 - accuracy: 0.6154\n",
      "\n",
      "Epoch 00013: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 1s 338ms/step - loss: 0.9621 - accuracy: 0.6875\n",
      "\n",
      "Epoch 00014: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 1s 333ms/step - loss: 0.5843 - accuracy: 0.6719\n",
      "\n",
      "Epoch 00015: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 1s 336ms/step - loss: 0.6556 - accuracy: 0.7188\n",
      "\n",
      "Epoch 00016: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 1s 334ms/step - loss: 0.7328 - accuracy: 0.6719\n",
      "\n",
      "Epoch 00017: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 359ms/step - loss: 0.7165 - accuracy: 0.7436\n",
      "\n",
      "Epoch 00018: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.5692 - accuracy: 0.7179\n",
      "\n",
      "Epoch 00019: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 354ms/step - loss: 0.4795 - accuracy: 0.7436\n",
      "\n",
      "Epoch 00020: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 1s 338ms/step - loss: 0.7471 - accuracy: 0.7500\n",
      "\n",
      "Epoch 00021: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.4596 - accuracy: 0.8205\n",
      "\n",
      "Epoch 00022: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 1s 338ms/step - loss: 0.6276 - accuracy: 0.8125\n",
      "\n",
      "Epoch 00023: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 378ms/step - loss: 0.5482 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00024: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.6923 - accuracy: 0.7436\n",
      "\n",
      "Epoch 00025: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.6858 - accuracy: 0.6923\n",
      "\n",
      "Epoch 00026: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 1s 333ms/step - loss: 0.5885 - accuracy: 0.7500\n",
      "\n",
      "Epoch 00027: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.6471 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00028: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 1s 333ms/step - loss: 0.5061 - accuracy: 0.8438\n",
      "\n",
      "Epoch 00029: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 1s 333ms/step - loss: 1.1287 - accuracy: 0.5625\n",
      "\n",
      "Epoch 00030: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 1s 329ms/step - loss: 0.7836 - accuracy: 0.7031\n",
      "\n",
      "Epoch 00031: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.7706 - accuracy: 0.6923\n",
      "\n",
      "Epoch 00032: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 349ms/step - loss: 0.6582 - accuracy: 0.6667\n",
      "\n",
      "Epoch 00033: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.8126 - accuracy: 0.7179\n",
      "\n",
      "Epoch 00034: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 1s 174ms/step - loss: 0.7889 - accuracy: 0.7436\n",
      "\n",
      "Epoch 00035: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 1s 333ms/step - loss: 0.7062 - accuracy: 0.7812\n",
      "\n",
      "Epoch 00036: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.8986 - accuracy: 0.6667\n",
      "\n",
      "Epoch 00037: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 1s 513ms/step - loss: 0.6726 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00038: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 1s 511ms/step - loss: 0.7117 - accuracy: 0.7188\n",
      "\n",
      "Epoch 00039: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.4963 - accuracy: 0.7812\n",
      "\n",
      "Epoch 00040: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.6709 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00041: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 385ms/step - loss: 0.2176 - accuracy: 0.8462\n",
      "\n",
      "Epoch 00042: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 1s 145ms/step - loss: 0.6149 - accuracy: 0.8205\n",
      "\n",
      "Epoch 00043: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 1s 160ms/step - loss: 0.5119 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00044: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6195 - accuracy: 0.5897\n",
      "\n",
      "Epoch 00045: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 1s 340ms/step - loss: 0.5054 - accuracy: 0.7344\n",
      "\n",
      "Epoch 00046: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.5141 - accuracy: 0.7179\n",
      "\n",
      "Epoch 00047: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 0.5087 - accuracy: 0.7436\n",
      "\n",
      "Epoch 00048: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.5519 - accuracy: 0.7656\n",
      "\n",
      "Epoch 00049: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.5890 - accuracy: 0.7949\n",
      "\n",
      "Epoch 00050: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 1s 377ms/step - loss: 0.4409 - accuracy: 0.8438\n",
      "\n",
      "Epoch 00051: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 1s 141ms/step - loss: 0.4182 - accuracy: 0.7949\n",
      "\n",
      "Epoch 00052: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 1s 517ms/step - loss: 0.6654 - accuracy: 0.6923\n",
      "\n",
      "Epoch 00053: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.5231 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00054: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.5305 - accuracy: 0.8205\n",
      "\n",
      "Epoch 00055: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.5879 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00056: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.7641 - accuracy: 0.6923\n",
      "\n",
      "Epoch 00057: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.6544 - accuracy: 0.7031\n",
      "\n",
      "Epoch 00058: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 390ms/step - loss: 0.6216 - accuracy: 0.6923\n",
      "\n",
      "Epoch 00059: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 1s 140ms/step - loss: 0.9424 - accuracy: 0.7179\n",
      "\n",
      "Epoch 00060: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 1s 351ms/step - loss: 0.3532 - accuracy: 0.8594\n",
      "\n",
      "Epoch 00061: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 1s 135ms/step - loss: 0.6807 - accuracy: 0.6923\n",
      "\n",
      "Epoch 00062: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 1s 352ms/step - loss: 0.6199 - accuracy: 0.7344\n",
      "\n",
      "Epoch 00063: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 1s 138ms/step - loss: 0.5379 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00064: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 390ms/step - loss: 0.5465 - accuracy: 0.7949\n",
      "\n",
      "Epoch 00065: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 1s 134ms/step - loss: 0.5360 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00066: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 1s 127ms/step - loss: 0.4153 - accuracy: 0.7949\n",
      "\n",
      "Epoch 00067: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 1s 348ms/step - loss: 0.5913 - accuracy: 0.7500\n",
      "\n",
      "Epoch 00068: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 1s 139ms/step - loss: 0.3045 - accuracy: 0.8462\n",
      "\n",
      "Epoch 00069: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 1s 350ms/step - loss: 0.3756 - accuracy: 0.8125\n",
      "\n",
      "Epoch 00070: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 1s 350ms/step - loss: 0.4732 - accuracy: 0.7969\n",
      "\n",
      "Epoch 00071: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 1s 133ms/step - loss: 0.4935 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00072: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 1s 136ms/step - loss: 0.5722 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00073: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 1s 348ms/step - loss: 0.4728 - accuracy: 0.7344\n",
      "\n",
      "Epoch 00074: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 1s 345ms/step - loss: 0.6490 - accuracy: 0.6719\n",
      "\n",
      "Epoch 00075: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 1s 128ms/step - loss: 0.4796 - accuracy: 0.7179\n",
      "\n",
      "Epoch 00076: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 1s 380ms/step - loss: 0.3527 - accuracy: 0.8125\n",
      "\n",
      "Epoch 00077: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 1s 368ms/step - loss: 0.3399 - accuracy: 0.8594\n",
      "\n",
      "Epoch 00078: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 1s 342ms/step - loss: 0.6299 - accuracy: 0.7031\n",
      "\n",
      "Epoch 00079: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 1s 339ms/step - loss: 0.4523 - accuracy: 0.7656\n",
      "\n",
      "Epoch 00080: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 382ms/step - loss: 0.4964 - accuracy: 0.7436\n",
      "\n",
      "Epoch 00081: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 1s 132ms/step - loss: 0.3993 - accuracy: 0.7949\n",
      "\n",
      "Epoch 00082: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 1s 135ms/step - loss: 0.6674 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00083: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 1s 132ms/step - loss: 0.3706 - accuracy: 0.7692\n",
      "\n",
      "Epoch 00084: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 1s 344ms/step - loss: 0.4151 - accuracy: 0.8125\n",
      "\n",
      "Epoch 00085: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 1s 136ms/step - loss: 0.7287 - accuracy: 0.5897\n",
      "\n",
      "Epoch 00086: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 1s 134ms/step - loss: 0.9151 - accuracy: 0.5385\n",
      "\n",
      "Epoch 00087: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 380ms/step - loss: 0.3659 - accuracy: 0.7949\n",
      "\n",
      "Epoch 00088: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 1s 356ms/step - loss: 0.4533 - accuracy: 0.7969\n",
      "\n",
      "Epoch 00089: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 1s 363ms/step - loss: 0.6008 - accuracy: 0.7656\n",
      "\n",
      "Epoch 00090: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 1s 131ms/step - loss: 0.5971 - accuracy: 0.7179\n",
      "\n",
      "Epoch 00091: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 1s 350ms/step - loss: 0.6134 - accuracy: 0.7500\n",
      "\n",
      "Epoch 00092: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 1s 138ms/step - loss: 0.3722 - accuracy: 0.7949\n",
      "\n",
      "Epoch 00093: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 1s 350ms/step - loss: 0.5402 - accuracy: 0.6719\n",
      "\n",
      "Epoch 00094: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 1s 130ms/step - loss: 0.4116 - accuracy: 0.8205\n",
      "\n",
      "Epoch 00095: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 376ms/step - loss: 0.6504 - accuracy: 0.7179\n",
      "\n",
      "Epoch 00096: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 1s 136ms/step - loss: 0.9169 - accuracy: 0.7179\n",
      "\n",
      "Epoch 00097: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 1s 138ms/step - loss: 0.6302 - accuracy: 0.7179\n",
      "\n",
      "Epoch 00098: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 381ms/step - loss: 0.6456 - accuracy: 0.6667\n",
      "\n",
      "Epoch 00099: saving model to EmotionDetectionModel2.h5\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 0.4149 - accuracy: 0.8718\n",
      "\n",
      "Epoch 00100: saving model to EmotionDetectionModel2.h5\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}